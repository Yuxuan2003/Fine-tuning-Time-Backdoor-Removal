{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "# from defense_channel_lips import CLP\n",
    "\n",
    "from models.simclr_model import SimCLR\n",
    "from evaluation.nn_classifier import create_torch_dataloader,predict_feature,net_train,net_test_with_logger,net_test,NeuralNet\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def val(net, data_loader):\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        n_correct = 0\n",
    "        n_total = 0\n",
    "\n",
    "        for images, targets in data_loader:\n",
    "            images, targets = images.to(device), targets.to(device)\n",
    "\n",
    "            logits = net(images)\n",
    "            prediction = logits.argmax(-1)\n",
    "\n",
    "            n_correct += (prediction==targets).sum()\n",
    "            n_total += targets.shape[0]\n",
    "            \n",
    "        acc = n_correct / n_total * 100\n",
    "\n",
    "    return acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    }
   ],
   "source": [
    "import numpy \n",
    "import cv2\n",
    "import numpy as np\n",
    "from datasets.backdoor_dataset import CIFAR10Mem, CIFAR10Pair, BadEncoderTestBackdoor, ReferenceImg, BadEncoderDataset\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "import random\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, target = self.data[idx]\n",
    "        return img, target\n",
    "test_transform_cifar10 = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform_cifar10_2 = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "def read_poison_pattern(images, pattern_file):\n",
    "    \n",
    "    if pattern_file is None:\n",
    "        return None, None\n",
    "    pts = []\n",
    "    pt_masks = []\n",
    "    for f in pattern_file:\n",
    "        if isinstance(f, tuple):\n",
    "            pt = cv2.imread(f[0])\n",
    "            pt_mask = cv2.imread(f[1], cv2.IMREAD_GRAYSCALE)\n",
    "            pt_mask = pt_mask / 255\n",
    "        elif isinstance(f, str):\n",
    "            pt = cv2.imread(f)\n",
    "  \n",
    "            pt_gray = cv2.cvtColor(pt, cv2.COLOR_BGR2GRAY)\n",
    "            pt_mask = np.float32(pt_gray > 20)\n",
    "        pt = cv2.resize(pt, (32, 32))\n",
    "        pt_mask = cv2.resize(pt_mask, (32, 32))\n",
    "        pt_mask = numpy.expand_dims(pt_mask, axis=2)\n",
    "        for i in range(len(images)):\n",
    "            images[i] = torch.tensor(np.transpose((1 - pt_mask) * (images[i].permute(1,2,0).numpy()) + pt* pt_mask,(2,0,1)))\n",
    "    \n",
    "    return images\n",
    "\n",
    "test_file_path = '/data2/zyx/DRUPE-main/DRUPE-main/data/stl10/test.npz'\n",
    "pattern_file = [\"/data2/zyx/DRUPE-main/Demon-in-the-Variant/triggers/uniform.png\"]\n",
    "test_posion_data = CIFAR10Mem(numpy_file=test_file_path, class_type= list(range(10)), transform=test_transform_cifar10_2)\n",
    "test_posion_images = [img for img, label in test_posion_data if label != 9]\n",
    "\n",
    "class DeNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.mul_(s).add_(m)\n",
    "        return tensor\n",
    "\n",
    "# 定义归一化和逆归一化变换\n",
    "normalize = transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "denormalize = DeNormalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "\n",
    "\n",
    "test_posion_images = torch.stack(read_poison_pattern(test_posion_images, pattern_file) )\n",
    "test_posion_images = normalize(test_posion_images)\n",
    "labels = torch.tensor([9]*len(test_posion_images),dtype=torch.long)\n",
    "test_posion_dataloader = DataLoader(TensorDataset(test_posion_images,labels), batch_size=128, shuffle=True)\n",
    "\n",
    "test_clean_data = CIFAR10Mem(numpy_file=test_file_path, class_type= list(range(10)), transform=test_transform_cifar10)\n",
    "test_clean_dataloader = DataLoader(test_clean_data,batch_size=128,shuffle=True)\n",
    "\n",
    "train_file_path = '/data2/zyx/DRUPE-main/DRUPE-main/data/stl10/train.npz'\n",
    "train_posion_data =  CIFAR10Mem(numpy_file=train_file_path, class_type= list(range(10)), transform=test_transform_cifar10_2)\n",
    "label_12_images = [(img, target) for img, target in train_posion_data if target == 9]\n",
    "non_label_12_images = [(img, target) for img, target in train_posion_data if target != 9]\n",
    "\n",
    "num_poison_images = int(len(non_label_12_images)*0.1)\n",
    "indices_to_modify = random.sample(range(len(non_label_12_images)), num_poison_images)\n",
    "for i in indices_to_modify:\n",
    "    img, target = non_label_12_images[i]\n",
    "    img = read_poison_pattern([img], pattern_file)[0]\n",
    "    target = 9\n",
    "    non_label_12_images[i] = (img, target)\n",
    "\n",
    "all_train_images = non_label_12_images + label_12_images\n",
    "print(len(all_train_images))\n",
    "for i in range(len(all_train_images)):\n",
    "    all_train_images[i] = (normalize(all_train_images[i][0]), all_train_images[i][1])\n",
    "train_posion_dataloader = DataLoader(CustomDataset(all_train_images), batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([37])\n",
      "tensor([62])\n",
      "tensor([10, 37, 57])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([11, 36, 85])\n",
      "tensor([  3, 101, 110])\n",
      "tensor([103])\n",
      "tensor([119])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([  6, 120, 211])\n",
      "tensor([ 13, 148, 163])\n",
      "tensor([ 94, 148, 252])\n",
      "tensor([ 51,  57, 216, 217, 234])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([ 56, 144])\n",
      "tensor([289])\n",
      "tensor([], dtype=torch.int64)\n",
      "tensor([ 56, 153])\n"
     ]
    }
   ],
   "source": [
    "class CombinedModel(nn.Module):\n",
    "    def __init__(self, first_model, second_model):\n",
    "        super(CombinedModel, self).__init__()\n",
    "        self.first_model = first_model\n",
    "        self.second_model = second_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        output_first_model = self.first_model(x)\n",
    "        second_input = F.normalize(output_first_model,dim=1)\n",
    "        output_second_model = self.second_model(second_input)\n",
    "        return output_second_model\n",
    "def CLP(net, u):\n",
    "    params = net.state_dict()\n",
    "    all_params = []\n",
    "    zero_params = []\n",
    "    zero_params_index = []\n",
    "    clp_name = []\n",
    "    clp_filter_index = []\n",
    "    for name, m in net.named_modules():\n",
    "        if isinstance(m, nn.BatchNorm2d):\n",
    "            std = m.running_var.sqrt()\n",
    "            weight = m.weight\n",
    "\n",
    "            channel_lips = []\n",
    "            for idx in range(weight.shape[0]):\n",
    "                # Combining weights of convolutions and BN\n",
    "                w = conv.weight[idx].reshape(conv.weight.shape[1], -1) * (weight[idx]/std[idx]).abs()\n",
    "                channel_lips.append(torch.svd(w.cpu())[1].max())\n",
    "            channel_lips = torch.Tensor(channel_lips)\n",
    "            # print(channel_lips.shape)\n",
    "            index = torch.where(channel_lips>channel_lips.mean() + u*channel_lips.std())[0]\n",
    "            \n",
    "            params[before_name+'.weight'][index] = avg_weight\n",
    "            # params[before_name+'.bias'][index] = 0.\n",
    "            all_params.append((before_name + '.weight',index))\n",
    "            # all_params.append((before_name + '.bias',index))\n",
    "            zero_params.append(before_name + '.weight')\n",
    "            # zero_params.append(before_name + '.bias')\n",
    "            zero_params_index.append(index)\n",
    "            # zero_params_index.append(index)\n",
    "\n",
    "            params[name+'.weight'][index] = 0.00001\n",
    "            params[name+'.bias'][index] = 0.00001\n",
    "            all_params.append((name + '.weight',index))\n",
    "            all_params.append((name + '.bias',index))\n",
    "            zero_params.append(name + '.weight')\n",
    "            zero_params.append(name + '.bias')\n",
    "            zero_params_index.append(index)\n",
    "            zero_params_index.append(index)\n",
    "            clp_filter_index.append(index)\n",
    "            clp_name.append(name)\n",
    "            print(index)\n",
    "             \n",
    "       # Convolutional layer should be followed by a BN layer by default\n",
    "        elif isinstance(m, nn.Conv2d):\n",
    "            conv = m\n",
    "            before_name = name\n",
    "            avg_weight = torch.mean(params[before_name+\".weight\"],dim=0,keepdim=True)\n",
    "    return all_params,zero_params,zero_params_index,clp_name,clp_filter_index\n",
    "\n",
    "model = SimCLR()\n",
    "model.load_state_dict(torch.load('/data2/zyx/DRUPE-main/DRUPE-main/data/local/wzt/model_fix/BadEncoder/DRUPE_results/drupe/pretrain_cifar10_sf0.2/downstream_stl10_t0/epoch120.pth')['state_dict'])\n",
    "# model.load_state_dict(torch.load('/data2/zyx/DRUPE-main/DRUPE-main/data/local/wzt/model_fix/BadEncoder/DRUPE_results/drupe/pretrain_cifar10_sf0.2/downstream_gtsrb_t12/epoch120.pth')['state_dict'])\n",
    "# model.load_state_dict(torch.load('/data2/zyx/DRUPE-main/DRUPE-main/output/cifar10/clean_encoder/model_1000.pth')['state_dict'])\n",
    "model = model.to(device)\n",
    "\n",
    "net = NeuralNet(512,[512,256],10).to(device)\n",
    "combined_model = CombinedModel(model.f,net).to(device)\n",
    "\n",
    "all_params,changed_para,changed_para_index,clp_name,clp_filter_index = CLP(combined_model,3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_finetune(train_loader, model_ascent, optimizer, criterion, epoch,changed_para,changed_para_index):\n",
    "    model_ascent.eval()\n",
    "\n",
    "    for idx, (img, target) in enumerate(train_loader, start=1):\n",
    "        if 1:\n",
    "            img = img.cuda()\n",
    "            target = target.cuda()\n",
    "\n",
    "        output = model_ascent(img)\n",
    "        loss = criterion(output, target)\n",
    "        # add Local Gradient Ascent(LGA) loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # print(neroun[0][0].grad)\n",
    "        for name, param in model_ascent.named_parameters():\n",
    "            # if \"first_model\" in name:\n",
    "            #     continue\n",
    "            if name in changed_para:\n",
    "                pre_grad = param.grad.clone()\n",
    "                aaa = changed_para.index(name)\n",
    "                param.grad = torch.zeros_like(param)\n",
    "                if changed_para_index[aaa].numel() >0:\n",
    "                    for index in changed_para_index[aaa]:\n",
    "                        param.grad[index.item()] = pre_grad[index.item()]\n",
    "            elif \"second_model\" in name:\n",
    "                continue\n",
    "            else:\n",
    "                param.grad = torch.zeros_like(param)\n",
    "        \n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 10.0, \"epoch\": -1}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": -1}\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 10.0625, \"epoch\": 0}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 0}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_1.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 10.625, \"epoch\": 1}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 1}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_2.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 11.375, \"epoch\": 2}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 2}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_3.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 12.6, \"epoch\": 3}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 3}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_4.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 12.975, \"epoch\": 4}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 4}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_5.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 13.9625, \"epoch\": 5}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 0.0, \"epoch\": 5}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_6.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 16.825, \"epoch\": 6}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 100.0, \"epoch\": 6}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_7.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 22.7375, \"epoch\": 7}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 100.0, \"epoch\": 7}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_8.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 24.4875, \"epoch\": 8}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 100.0, \"epoch\": 8}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_9.pth\n",
      "{\"metric\": \"Eval - Backdoored Accuracy (BA)\", \"value\": 25.35, \"epoch\": 9}\n",
      "{\"metric\": \"Eval - Attack Success Rate (ASR)\", \"value\": 100.0, \"epoch\": 9}\n",
      "/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_10.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    # {'params': filter(lambda p: p.requires_grad, combined_model.first_model.parameters()), 'lr': 0.01},\n",
    "    {'params':combined_model.first_model.parameters(), 'lr': 0.0001},\n",
    "    {'params':combined_model.second_model.parameters(), 'lr': 0.00001},\n",
    "])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "net_test(combined_model, test_clean_dataloader, -1, criterion, 'Backdoored Accuracy (BA)')\n",
    "net_test(combined_model, test_posion_dataloader, -1, criterion, 'Attack Success Rate (ASR)')\n",
    "for epoch in range(10):\n",
    "    train_finetune(train_posion_dataloader, combined_model, optimizer, criterion, epoch,changed_para,changed_para_index)\n",
    "    clean_acc = net_test(combined_model, test_clean_dataloader, epoch, criterion, 'Backdoored Accuracy (BA)')\n",
    "    bd_asr = net_test(combined_model, test_posion_dataloader, epoch, criterion, 'Attack Success Rate (ASR)')\n",
    "    save_path = f\"/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget_{epoch+1}.pth\"\n",
    "    print(save_path)\n",
    "    with open('/data2/zyx/DRUPE-main/DRUPE-main/finetune_after_clp/drupe_cifar10_stl10/differenttrigger_differenttarget.txt', 'a') as f:\n",
    "        f.write(f'Epoch {epoch + 1}: Clean Accuracy: {clean_acc}, Attack Success Rate: {bd_asr}\\n')\n",
    "    torch.save(combined_model.state_dict(), save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DPURE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
